{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lfw64x48bw_train.ipynb","provenance":[{"file_id":"1htJYNlZs0qwagGMTlzv-MEdIk3CiDCcD","timestamp":1575405162202}],"collapsed_sections":["YHx43RJIn_1q","PsKiKjbGai8K","caF0H950aqUG","VLDlQZYdoNIP"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yTW77kXOHeB2","colab_type":"text"},"source":["# GAN - LFW - 64×48 px (grayscale)\n","Generative Adversarial Network for generating images of faces from Labeled Faces in the Wild database - code for training GAN.\n","\n","Developed by Daniel Konečný"]},{"cell_type":"markdown","metadata":{"id":"YHx43RJIn_1q","colab_type":"text"},"source":["## Initialize\n","Defines the basic libraries and initializes global variables needed in all codes. Connects the code to data source - Google Drive."]},{"cell_type":"code","metadata":{"id":"3aACgQek9nsg","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","\n","dataset = 'lfw'\n","x_dimension = 64\n","y_dimension = 48\n","note = 'bw'\n","\n","project_name = f'{dataset}{x_dimension}x{y_dimension}{note}'\n","project_path = '/'\n","grid_path = f'{project_path}grids/'\n","model_path = f'{project_path}models/'\n","\n","latent_dimension = 128\n","batch_size = 256"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PsKiKjbGai8K","colab_type":"text"},"source":["## Training functions\n","All functions necessary for training, initialize before."]},{"cell_type":"code","metadata":{"id":"rZwfgHFec4Xw","colab_type":"code","colab":{}},"source":["import time\n","from tensorflow.keras import layers\n","from matplotlib import pyplot\n","from sklearn.datasets import fetch_lfw_people\n","\n","\n","def get_discriminator(image_shape=(x_dimension, y_dimension, 1)):\n","\tdiscriminator = tf.keras.Sequential()\n"," \n","\tdiscriminator.add(layers.Conv2D(32, (1, 1), padding='same', input_shape=image_shape))\n","\tdiscriminator.add(layers.LeakyReLU(alpha=0.2))\n","\tdiscriminator.add(layers.Dropout(0.4))\n","\t# None is the batch size.\n","\tassert discriminator.output_shape == (None, x_dimension, y_dimension, 32)\n","\n","\tdiscriminator.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n","\tdiscriminator.add(layers.LeakyReLU(alpha=0.2))\n","\tdiscriminator.add(layers.Dropout(0.4))\n","\tassert discriminator.output_shape == (None, x_dimension//2, y_dimension//2, 64)\n"," \n","\tdiscriminator.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n","\tdiscriminator.add(layers.LeakyReLU(alpha=0.2))\n","\tdiscriminator.add(layers.Dropout(0.4))\n","\tassert discriminator.output_shape == (None, x_dimension//4, y_dimension//4, 64)\n"," \n","\tdiscriminator.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n","\tdiscriminator.add(layers.LeakyReLU(alpha=0.2))\n","\tdiscriminator.add(layers.Dropout(0.4))\n","\tassert discriminator.output_shape == (None, x_dimension//8, y_dimension//8, 128)\n"," \n","\tdiscriminator.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n","\tdiscriminator.add(layers.LeakyReLU(alpha=0.2))\n","\tdiscriminator.add(layers.Dropout(0.4))\n","\tassert discriminator.output_shape == (None, x_dimension//16, y_dimension//16, 128)\n"," \n","\tdiscriminator.add(layers.Flatten())\n","\tdiscriminator.add(layers.Dense(1, activation='sigmoid'))\n"," \n","\tdiscriminator_optimizer = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n","\tdiscriminator.compile(loss='binary_crossentropy',\n","\t                      optimizer=discriminator_optimizer,\n","\t\t\t\t\t\t  metrics=['accuracy'])\n"," \n","\treturn discriminator\n","\n","\n","def get_generator():\n","\tgenerator = tf.keras.Sequential()\n","\t\n","\tgenerator.add(layers.Dense(x_dimension//16 * y_dimension//16 * 128, input_dim=latent_dimension))\n","\tgenerator.add(layers.LeakyReLU(alpha=0.2))\n","\tgenerator.add(layers.Reshape((x_dimension//16, y_dimension//16, 128)))\n","\tassert generator.output_shape == (None, x_dimension//16, y_dimension//16, 128)\n","\n","\tgenerator.add(layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same'))\n","\tgenerator.add(layers.LeakyReLU(alpha=0.2))\n","\tassert generator.output_shape == (None, x_dimension//8, y_dimension//8, 128)\n","\n","\tgenerator.add(layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same'))\n","\tgenerator.add(layers.LeakyReLU(alpha=0.2))\n","\tassert generator.output_shape == (None, x_dimension//4, y_dimension//4, 64)\n"," \n","\tgenerator.add(layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same'))\n","\tgenerator.add(layers.LeakyReLU(alpha=0.2))\n","\tassert generator.output_shape == (None, x_dimension//2, y_dimension//2, 64)\n"," \n","\tgenerator.add(layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same'))\n","\tgenerator.add(layers.LeakyReLU(alpha=0.2))\n","\tassert generator.output_shape == (None, x_dimension, y_dimension, 32)\n"," \n","\tgenerator.add(layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same'))\n","\tassert generator.output_shape == (None, x_dimension, y_dimension, 1)\n","\n","\treturn generator\n","\n","\n","def get_gan(generator, discriminator):\n","\tgan = tf.keras.Sequential()\n"," \n","\tgan.add(generator)\n","\tdiscriminator.trainable = False\n","\tgan.add(discriminator)\n","\n","\tgan_optimizer = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n","\tgan.compile(loss='binary_crossentropy', optimizer=gan_optimizer)\n","\n","\treturn gan\n","\n","\n","def get_dataset():\n","\tdataset = fetch_lfw_people()\n","\n","\tresized_dataset = np.empty((len(dataset.images), x_dimension, y_dimension))\n","\n","\t# Iterate over all images.\n","\tfor image_index in range(len(dataset.images)):\n","\t\tresized_dataset[image_index] = np.empty((x_dimension, y_dimension))\n","\n","\t\tfor row_index in range(len(dataset.images[image_index])):\n","\t\t\t# Duplicate the first column.\n","\t\t\tresized_dataset[image_index][row_index+1][0] = \\\n","\t\t\tdataset.images[image_index][row_index][0]\n","\t\t\tfor column_index in range(len(dataset.images[image_index][0])):\n","\t\t\t\t# Copy the rest of the values.\n","\t\t\t\tresized_dataset[image_index][row_index+1][column_index+1] = \\\n","\t\t\t\tdataset.images[image_index][row_index][column_index]\n","\n","\t\t# Duplicate the first and last row.\n","\t\tresized_dataset[image_index][0] = resized_dataset[image_index][1]\n","\t\tresized_dataset[image_index][-1] = resized_dataset[image_index][-2]\n","\n","\tdataset = np.expand_dims(resized_dataset, axis=-1)\n","\tdataset = dataset.astype('float32') / 255.0\n","\treturn dataset\n","\n","\n","def get_latent_points(sample_count=1):\n","\tlatents = np.empty((sample_count, latent_dimension))\n","\n","\tfor latents_index in range(sample_count):\n","\t\trandoms = np.random.normal(0, 1, latent_dimension)\n","\t\tnormalizer = np.sum(randoms**2)**0.5\n","\t\tlatent = randoms/normalizer\n","\t\tlatents[latents_index] = latent\n","\t\n","\treturn latents\n","\n","\n","def get_generated_images(generator, image_count):\n","\tgenerated_latents = get_latent_points(image_count)\n","\tgenerated_images = generator.predict(generated_latents)\n","\tgenerated_labels = np.zeros((image_count, 1))\n","\treturn generated_images, generated_labels\n","\n","\n","def get_real_images(dataset, image_count):\n","\treal_randoms = np.random.randint(0, dataset.shape[0], image_count)\n","\treal_images = dataset[real_randoms]\n","\treal_labels = np.ones((image_count, 1))\n","\treturn real_images, real_labels\n","\n","\n","def create_image_grid(examples, image_grid_size, epoch_index):\n","\tfor grid_index in range(image_grid_size * image_grid_size):\n","\t\tpyplot.subplot(image_grid_size, image_grid_size, 1 + grid_index)\n","\t\tpyplot.axis('off')\n","\t\tpyplot.imshow(examples[grid_index, :, :, 0], cmap='gray')\n","\tpyplot.savefig(f'{grid_path}{project_name}_grid{epoch_index+1:04d}.png')\n","\tpyplot.show()\n","\n","\n","def evaluate_model(epoch_index, generator, discriminator, dataset,\n","                   sample_count=100, image_grid_size=5):\n","\treal_images, real_labels = get_real_images(dataset, sample_count)\n","\t_, real_accuracy = discriminator.evaluate(real_images, real_labels, verbose=0)\n","\t\n","\tgenerated_images, generated_labels = get_generated_images(generator, sample_count)\n","\t_, generated_accuracy = discriminator.evaluate(generated_images, generated_labels, verbose=0)\n","\t\n","\tprint(f'Accuracy real: {real_accuracy*100:.0f} %, generated: {generated_accuracy*100:.0f} %')\n","\tgenerator.save(f'{model_path}{project_name}_generator{epoch_index+1:04d}.h5')\n"," \n","\tif (epoch_index + 1) % 10 == 0:\n","\t\tdisplay_latent = get_latent_points(image_grid_size*image_grid_size)\n","\t\tdisplay_images = generator.predict(display_latent)\n","\t\tcreate_image_grid(display_images, image_grid_size, epoch_index)\n","\n","\n","def train(generator, discriminator, gan, dataset, epoch_count):\n","\tbatch_count = dataset.shape[0] // batch_size\n","\n","\tfor epoch_index in range(epoch_count):\n","\t\tepoch_start_time = time.time()\n","  \n","\t\tfor batch_index in range(batch_count):\n","\t\t\t# Discriminator training.\n","\t\t\treal_images, real_labels = get_real_images(dataset, batch_size // 2)\n","\t\t\tgenerated_images, generated_labels = get_generated_images(generator, batch_size // 2)\n","\t\t\timages, labels = np.vstack((real_images, generated_images)), np.vstack((real_labels, generated_labels))\n","\t\t\tdiscriminator_loss, _ = discriminator.train_on_batch(images, labels)\n","   \n","\t\t\t# Generator training.\n","\t\t\tgenerated_latent = get_latent_points(batch_size)\n","\t\t\tgenerated_labels = np.ones((batch_size, 1))\n","\t\t\tgenerator_loss = gan.train_on_batch(generated_latent, generated_labels)\n","   \n","\t\t\tprint(f'- {epoch_index+1}, {batch_index+1}/{batch_count}, d={discriminator_loss:.3f}, g={generator_loss:.3f}')\n","   \n","\t\tevaluate_model(epoch_index, generator, discriminator, dataset)\n","\n","\t\tepoch_end_time = time.time()\n","\t\tprint(f'Epoch {epoch_index+1} finished in {epoch_end_time - epoch_start_time} s.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"caF0H950aqUG","colab_type":"text"},"source":["## Training\n","Set epoch count and launch for training of the GAN."]},{"cell_type":"code","metadata":{"id":"ZFgxhr2OaGC9","colab_type":"code","colab":{}},"source":["epoch_count = 1000\n","\n","print(\"Creating GAN...\")\n","discriminator = get_discriminator()\n","generator = get_generator()\n","gan = get_gan(generator, discriminator)\n","\n","print(\"Loading dataset...\")\n","dataset = get_dataset()\n","\n","print(\"Training...\")\n","train(generator, discriminator, gan, dataset, epoch_count)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VLDlQZYdoNIP","colab_type":"text"},"source":["## Summary\n","Launch to display the summary of Discriminator and Generator models."]},{"cell_type":"code","metadata":{"id":"pNC0fhTQoupW","colab_type":"code","colab":{}},"source":["print('\\x1b[0;32;40m' + 'DISCRIMINATOR' + '\\x1b[0m')\n","discriminator = get_discriminator()\n","print(f'{discriminator.summary()}\\n')\n","\n","print('\\x1b[0;32;40m' + 'GENERATOR' + '\\x1b[0m')\n","generator = get_generator()\n","print(f'{generator.summary()}\\n')"],"execution_count":0,"outputs":[]}]}