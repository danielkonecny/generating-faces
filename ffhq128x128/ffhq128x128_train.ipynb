{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ffhq128x128_train.ipynb","provenance":[{"file_id":"1htJYNlZs0qwagGMTlzv-MEdIk3CiDCcD","timestamp":1575405162202}],"collapsed_sections":["v3uProvHt6_R","lRMMoDwa6dMm","PsKiKjbGai8K","caF0H950aqUG","VLDlQZYdoNIP"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yTW77kXOHeB2","colab_type":"text"},"source":["# GAN - FFHQ - 128×128 px\n","Generative Adversarial Network for generating images of faces from Flickr-Faces-HQ database - code for training GAN.\n","\n","Developed by Daniel Konečný"]},{"cell_type":"markdown","metadata":{"id":"v3uProvHt6_R","colab_type":"text"},"source":["## Initialize\n","Defines the basic libraries and initializes global variables needed in all codes. Connects the code to data source - Google Drive."]},{"cell_type":"code","metadata":{"id":"3aACgQek9nsg","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","\n","dataset = 'ffhq'\n","x_dimension = 128\n","y_dimension = 128\n","note = ''\n","\n","project_name = f'{dataset}{x_dimension}x{y_dimension}{note}'\n","project_path = '/'\n","dataset_path = f'{project_path}datasets/'\n","grid_path = f'{project_path}grids/'\n","raw_path = f'{project_path}raw/'\n","weight_path = f'{project_path}weights/'\n","\n","latent_dimension = 256\n","batch_size = 256"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lRMMoDwa6dMm","colab_type":"text"},"source":["## Load and preprocess training data\n","Loads all the images used for training and saves them as numpy nd-arrays.\n","\n","**To prepare the dataset, it is necessary to have the original images saved in `raw` folder (in subfolders with size of 1000).**"]},{"cell_type":"markdown","metadata":{"id":"UJOrxfsNxTWL","colab_type":"text"},"source":["### Compressed files (NPZ)"]},{"cell_type":"code","metadata":{"id":"0GIn3zL6cPvh","colab_type":"code","colab":{}},"source":["import matplotlib.image as mpimg\n","\n","dataset_start = 0\n","dataset_size = 70000\n","dataset_step = 10000\n","raw_folder_size = 1000\n","dataset = np.empty((dataset_step, x_dimension, y_dimension, 3))\n","\n","for dataset_index in range(dataset_start, dataset_size, dataset_step):\n","    for image_index in range(dataset_index, dataset_index + dataset_step):\n","        dataset[image_index - dataset_index] = mpimg.imread(\n","            f'{raw_path}'\n","            f'{(image_index//raw_folder_size)*raw_folder_size:05d}/'\n","            f'{image_index:05d}.png')\n","        if (image_index - dataset_index) % 100 == 0:\n","            print(f'Dataset index: {dataset_index}, Image index: {image_index}')\n","    np.savez_compressed(f'{dataset_path}{project_name}_dataset{dataset_index:05d}.npz', dataset)\n","    print(f'Dataset exported: {save_file}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dbjFAl03xatb","colab_type":"text"},"source":["### Non-compressed files (NPY)"]},{"cell_type":"code","metadata":{"id":"KvztY7i_J_DC","colab_type":"code","colab":{}},"source":["import matplotlib.image as mpimg\n","\n","dataset_start = 0\n","dataset_size = 70000\n","dataset_step = 14000\n","raw_folder_size = 1000\n","dataset = np.empty((dataset_step, x_dimension, y_dimension, 3))\n","\n","for dataset_index in range(dataset_start, dataset_size, dataset_step):\n","    for image_index in range(dataset_index, dataset_index + dataset_step):\n","        dataset[image_index - dataset_index] = mpimg.imread(\n","            f'{raw_path}'\n","            f'{(image_index//raw_folder_size)*raw_folder_size:05d}/'\n","            f'{image_index:05d}.png')\n","        if (image_index - dataset_index) % 100 == 0:\n","            print(f'Image {image_index:05d} processed.')\n","    np.save(f'{dataset_path}{project_name}_dataset{dataset_index:05d}.npy', dataset)\n","    print(f'Dataset {dataset_index:05d} exported.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Blz_ANCml1pp","colab_type":"text"},"source":["### Convert preloaded NPZ to NPY"]},{"cell_type":"code","metadata":{"id":"qdL4ys5ajtf6","colab_type":"code","colab":{}},"source":["start = 0\n","size = 70000\n","dataset_step = 1400\n","dataset = np.empty((dataset_step, x_dimension, y_dimension, 3))\n","loaded_dataset_step = 10000\n","loaded_dataset = np.empty((loaded_dataset_step, x_dimension, y_dimension, 3))\n","\n","dataset_index = start\n","for loaded_dataset_index in range ((start // loaded_dataset_step) * loaded_dataset_step, size, loaded_dataset_step):\n","    loaded_samples = np.load(f'{dataset_path}{project_name}_dataset{loaded_dataset_index:05d}.npz')\n","    loaded_dataset = loaded_samples['arr_0']\n","    print(f'Dataset {loaded_dataset_index:05d} loaded.')\n","    while dataset_index < loaded_dataset_index + loaded_dataset_step:\n","        dataset[dataset_index % dataset_step] = loaded_dataset[dataset_index - loaded_dataset_index]\n","        dataset_index += 1\n","        if dataset_index % dataset_step == 0:\n","            np.save(f'{dataset_path}{project_name}_dataset'\n","                    f'{(dataset_index//dataset_step-1)*dataset_step:05d}.npy', dataset)\n","            print(f'Dataset {(dataset_index//dataset_step-1)*dataset_step:05d} exported.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PsKiKjbGai8K","colab_type":"text"},"source":["## Training functions\n","All functions necessary for training, initialize before."]},{"cell_type":"code","metadata":{"id":"rZwfgHFec4Xw","colab_type":"code","colab":{}},"source":["import time\n","from tensorflow.keras import layers\n","from matplotlib import pyplot\n","from sklearn.datasets import fetch_lfw_people\n","\n","\n","dataset = None\n","dataset_size = 70000\n","dataset_step = 1400\n","\n","\n","def get_discriminator(image_shape=(x_dimension, y_dimension, 3)):\n","\tdiscriminator = tf.keras.Sequential()\n"," \n","\tdiscriminator.add(layers.Conv2D(32, (1, 1), padding='same', input_shape=image_shape))\n","\tdiscriminator.add(layers.LeakyReLU(alpha=0.2))\n","\tdiscriminator.add(layers.Dropout(0.4))\n","\t# None is the batch size.\n","\tassert discriminator.output_shape == (None, x_dimension, y_dimension, 32)\n","\n","\tdiscriminator.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n","\tdiscriminator.add(layers.LeakyReLU(alpha=0.2))\n","\tdiscriminator.add(layers.Dropout(0.4))\n","\tassert discriminator.output_shape == (None, x_dimension//2, y_dimension//2, 64)\n"," \n","\tdiscriminator.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n","\tdiscriminator.add(layers.LeakyReLU(alpha=0.2))\n","\tdiscriminator.add(layers.Dropout(0.4))\n","\tassert discriminator.output_shape == (None, x_dimension//4, y_dimension//4, 128)\n"," \n","\tdiscriminator.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n","\tdiscriminator.add(layers.LeakyReLU(alpha=0.2))\n","\tdiscriminator.add(layers.Dropout(0.4))\n","\tassert discriminator.output_shape == (None, x_dimension//8, y_dimension//8, 128)\n"," \n","\tdiscriminator.add(layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same'))\n","\tdiscriminator.add(layers.LeakyReLU(alpha=0.2))\n","\tdiscriminator.add(layers.Dropout(0.4))\n","\tassert discriminator.output_shape == (None, x_dimension//16, y_dimension//16, 256)\n"," \n","\tdiscriminator.add(layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same'))\n","\tdiscriminator.add(layers.LeakyReLU(alpha=0.2))\n","\tdiscriminator.add(layers.Dropout(0.4))\n","\tassert discriminator.output_shape == (None, x_dimension//32, y_dimension//32, 256)\n"," \n","\tdiscriminator.add(layers.Flatten())\n","\tdiscriminator.add(layers.Dense(1, activation='sigmoid'))\n"," \n","\tdiscriminator_optimizer = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n","\tdiscriminator.compile(loss='binary_crossentropy',\n","\t                      optimizer=discriminator_optimizer,\n","\t\t\t\t\t\t  metrics=['accuracy'])\n"," \n","\treturn discriminator\n","\n","\n","def get_generator():\n","\tgenerator = tf.keras.Sequential()\n","\t\n","\tgenerator.add(layers.Dense(x_dimension//32 * y_dimension//32 * 256, input_dim=latent_dimension))\n","\tgenerator.add(layers.LeakyReLU(alpha=0.2))\n","\tgenerator.add(layers.Reshape((x_dimension//32, y_dimension//32, 256)))\n","\tassert generator.output_shape == (None, x_dimension//32, y_dimension//32, 256)\n","\n","\tgenerator.add(layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same'))\n","\tgenerator.add(layers.LeakyReLU(alpha=0.2))\n","\tassert generator.output_shape == (None, x_dimension//16, y_dimension//16, 256)\n","\n","\tgenerator.add(layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same'))\n","\tgenerator.add(layers.LeakyReLU(alpha=0.2))\n","\tassert generator.output_shape == (None, x_dimension//8, y_dimension//8, 128)\n","\n","\tgenerator.add(layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same'))\n","\tgenerator.add(layers.LeakyReLU(alpha=0.2))\n","\tassert generator.output_shape == (None, x_dimension//4, y_dimension//4, 128)\n"," \n","\tgenerator.add(layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same'))\n","\tgenerator.add(layers.LeakyReLU(alpha=0.2))\n","\tassert generator.output_shape == (None, x_dimension//2, y_dimension//2, 64)\n"," \n","\tgenerator.add(layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same'))\n","\tgenerator.add(layers.LeakyReLU(alpha=0.2))\n","\tassert generator.output_shape == (None, x_dimension, y_dimension, 32)\n"," \n","\tgenerator.add(layers.Conv2D(3, (1, 1), activation='sigmoid', padding='same'))\n","\tassert generator.output_shape == (None, x_dimension, y_dimension, 3)\n","\n","\treturn generator\n","\n","\n","def get_gan(generator, discriminator):\n","\tgan = tf.keras.Sequential()\n"," \n","\tgan.add(generator)\n","\tdiscriminator.trainable = False\n","\tgan.add(discriminator)\n","\n","\tgan_optimizer = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n","\tgan.compile(loss='binary_crossentropy', optimizer=gan_optimizer)\n","\n","\treturn gan\n","\n","\n","def get_latent_points(sample_count=1):\n","\tlatents = np.empty((sample_count, latent_dimension))\n","\n","\tfor latents_index in range(sample_count):\n","\t\trandoms = np.random.normal(0, 1, latent_dimension)\n","\t\tnormalizer = np.sum(randoms**2)**0.5\n","\t\tlatent = randoms/normalizer\n","\t\tlatents[latents_index] = latent\n","\t\n","\treturn latents\n","\n","\n","def get_generated_images(generator, image_count):\n","\tgenerated_latents = get_latent_points(image_count)\n","\tgenerated_images = generator.predict(generated_latents)\n","\tgenerated_labels = np.zeros((image_count, 1))\n","\treturn generated_images, generated_labels\n","\n","\n","def get_real_images(image_index, image_count):\n","\tglobal dataset\n","\tif image_index % dataset_step == 0:\n","\t\tdataset_load_start_time = time.time()\n","\t\tprint(f'Loading {image_index:05d} dataset...')\n","\t\tdataset = np.load(f'{dataset_path}{project_name}_dataset{image_index:05d}.npy')\n","\t\tdataset_load_end_time = time.time()\n","\t\tprint(f'Dataset loaded in {dataset_load_end_time - dataset_load_start_time} s.')\n","\tstart_index = image_index%dataset_step\n","\tend_index = (image_index+image_count)%dataset_step\n","\tif end_index == 0:\n","\t\tend_index = dataset_step\n","\treal_images = dataset[start_index:end_index, :, :, :]\n","\tnp.random.shuffle(real_images)\n","\treal_labels = np.ones((image_count, 1))\n","\treturn real_images, real_labels\n","\n","\n","def get_batch(image_index, generator):\n","\timage_count = batch_size // 2\n","\tif dataset_step - image_index % dataset_step < image_count:\n","\t\timage_count = dataset_step - image_index % dataset_step\n","\n","\treal_images, real_labels = get_real_images(image_index, image_count)\n","\tgenerated_images, generated_labels = get_generated_images(generator, batch_size-image_count)\n","\n","\treturn np.vstack((real_images, generated_images)), np.vstack((real_labels, generated_labels))\n","\n","\n","def create_image_grid(examples, image_grid_size, epoch_index):\n","\tfor grid_index in range(image_grid_size * image_grid_size):\n","\t\tpyplot.subplot(image_grid_size, image_grid_size, 1 + grid_index)\n","\t\tpyplot.axis('off')\n","\t\tpyplot.imshow(examples[grid_index])\n","\tpyplot.savefig(f'{grid_path}{project_name}_grid{epoch_index+1:04d}.png')\n","\tpyplot.show()\n","\n","\n","def evaluate_model(generator, discriminator, sample_count=100):\n","\trandom_index = np.random.randint(dataset_size-dataset_step+1, dataset_size-sample_count)\n"," \n","\treal_images, real_labels = get_real_images(random_index, sample_count)\n","\t_, real_accuracy = discriminator.evaluate(real_images, real_labels, verbose=0)\n","\t\n","\tgenerated_images, generated_labels = get_generated_images(generator, sample_count)\n","\t_, generated_accuracy = discriminator.evaluate(generated_images, generated_labels, verbose=0)\n","\t\n","\tprint(f'Accuracy real: {real_accuracy*100:.0f} %, generated: {generated_accuracy*100:.0f} %')\n","\n","\n","# train the generator and discriminator\n","def train(generator, discriminator, gan, first_epoch, epoch_count):\n","\timage_grid_size = 5\n","\n","\tfor epoch_index in range(first_epoch, epoch_count):\n","\t\tepoch_start_time = time.time()\n","\t\timage_index = 0\n","\t\twhile image_index < dataset_size:\n","\t\t\t# Discriminator training.\n","\t\t\timages, labels = get_batch(image_index, generator)\n","\t\t\tdiscriminator_loss, _ = discriminator.train_on_batch(images, labels)\n","   \n","\t\t\t# Generator training.\n","\t\t\tlatent_points = get_latent_points(batch_size)\n","\t\t\tgenerated_labels = np.ones((batch_size, 1))\n","\t\t\tgenerator_loss = gan.train_on_batch(latent_points, generated_labels)\n","\n","\t\t\tprint(f'>{epoch_index+1}, {image_index}/{dataset_size}, d={discriminator_loss:.3f}, g={generator_loss:.3f}')\n","\n","\t\t\timage_index += batch_size // 2\n","\t\t\tif image_index % dataset_step < batch_size // 2:\n","\t\t\t\tprint(f'Changing image index manually from {image_index} to {image_index // dataset_step * dataset_step}')\n","\t\t\t\timage_index = image_index // dataset_step * dataset_step\n","   \n","\t\tevaluate_model(generator, discriminator)\n","\t\tgenerator.save_weights(f'{weight_path}{project_name}_generator{epoch_index+1:04d}.h5')\n","\t\tdiscriminator.save_weights(f'{weight_path}{project_name}_discriminator{epoch_index+1:04d}.h5')\n","\t\tgan.save_weights(f'{weight_path}{project_name}_gan{epoch_index+1:04d}.h5')\n","  \n","\t\tif (epoch_index + 1) % 10 == 0:\n","\t\t\tdisplay_latent = get_latent_points(image_grid_size*image_grid_size)\n","\t\t\tdisplay_images = generator.predict(display_latent)\n","\t\t\tcreate_image_grid(display_images, image_grid_size, epoch_index)\n","\t\t\n","\t\tepoch_end_time = time.time()\n","\t\tprint(f'Epoch {epoch_index+1} finished in {epoch_end_time - epoch_start_time} s.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"caF0H950aqUG","colab_type":"text"},"source":["## Training\n","Set epoch count and launch for training of the GAN."]},{"cell_type":"code","metadata":{"id":"ZFgxhr2OaGC9","colab_type":"code","colab":{}},"source":["epoch_count = 1000\n","first_epoch = 1000\n","\n","print(\"Creating GAN...\")\n","discriminator = get_discriminator()\n","generator = get_generator()\n","gan = get_gan(generator, discriminator)\n","if first_epoch > 0:\n","    discriminator.load_weights(f'{weight_path}{project_name}_discriminator{first_epoch:04d}.h5')\n","    generator.load_weights(f'{weight_path}{project_name}_generator{first_epoch:04d}.h5')\n","    gan.load_weights(f'{weight_path}{project_name}_gan{first_epoch:04d}.h5')\n","\n","print(\"Training...\")\n","train(generator, discriminator, gan, first_epoch, epoch_count)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VLDlQZYdoNIP","colab_type":"text"},"source":["## Summary\n","Launch to display the summary of Discriminator and Generator models."]},{"cell_type":"code","metadata":{"id":"pNC0fhTQoupW","colab_type":"code","colab":{}},"source":["print('\\x1b[0;32;40m' + 'DISCRIMINATOR' + '\\x1b[0m')\n","discriminator = get_discriminator()\n","print(f'{discriminator.summary()}\\n')\n","\n","print('\\x1b[0;32;40m' + 'GENERATOR' + '\\x1b[0m')\n","generator = get_generator()\n","print(f'{generator.summary()}\\n')"],"execution_count":0,"outputs":[]}]}